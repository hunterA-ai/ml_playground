{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Package Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "## Custom Module Imports\n",
    "from activation_functions.SoftMax import SoftMax\n",
    "from activation_functions.ReLU import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class nn_layer:\n",
    "    \"\"\"\n",
    "    Represents a weight matrix (rows, cols) = (num_neurons, input_size)\n",
    "    num_neurons is the number of neurons we wish to put in this layer\n",
    "    input_size is the fixed value defined by the last layer's outputs\n",
    "\n",
    "    The relationship between input size and number of neurons for multiple layers is ---\n",
    "    input_size = num_neurons_prev\n",
    "    input_size_next/output_size = num_neurons\n",
    "    ... etc\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_neurons, activ_func):\n",
    "        # self.weight_matrix = np.array([np.random.rand(input_size) for _ in range(num_neurons)])\n",
    "        # Above line has been upgraded to line below\n",
    "        self.weight_matrix = np.random.randn(num_neurons, input_size)\n",
    "        self.bias_vector = np.random.rand(num_neurons)\n",
    "        self.activation_func = activ_func\n",
    "\n",
    "    def batch_input(self, input_matrix):\n",
    "        \"\"\"\n",
    "        Returns the matrix product [input_matrix] * [weight_matrix]^T of dimensions\n",
    "        (num_inputs, input_size) * (input_size, num_neurons) = (num_inputs, num_neurons)\n",
    "        Where the output columns of the matrix are the output of the i^{th} layer of neurons\n",
    "\n",
    "\n",
    "        (num_inputs, num_neurons) + (num_neurons) is X + bias, where the bias is added row-wise (to each row/data instance)\n",
    "        \"\"\"\n",
    "        raw_batch_output = np.dot(input_matrix, self.weight_matrix.T) + self.bias_vector\n",
    "        batch_output = self.activation_func.forward(raw_batch_output)\n",
    "        return batch_output\n",
    "    \n",
    "\n",
    "class simple_neural_network:\n",
    "    \"\"\"\n",
    "    Represents a neural network as an array of 'nn_layer' objects\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        self.nn_array = []\n",
    "        self.input_size = input_size\n",
    "\n",
    "\n",
    "    def add_layer(self, num_neurons, activ_func):\n",
    "        \"\"\"\n",
    "        New layer must have input size corresponding to previous layer's output size\n",
    "        num_neurons - is the number of neurons in the current layer\n",
    "        activ_func - is the activation function that should be applied to the outputs of this layer\n",
    "        \"\"\"\n",
    "        if(len(self.nn_array) == 0):\n",
    "            self.nn_array.append(nn_layer(self.input_size, num_neurons, activ_func))\n",
    "        else:\n",
    "            prev_output_size = self.nn_array[-1].weight_matrix.shape[0]\n",
    "            self.nn_array.append(nn_layer(\n",
    "                input_size = prev_output_size, \n",
    "                num_neurons = num_neurons, \n",
    "                activ_func=activ_func))\n",
    "\n",
    "\n",
    "    def describe_network(self):\n",
    "        # weight matrix shape is (num_neurons, input_size)\n",
    "        for layer in self.nn_array:\n",
    "            print(layer)\n",
    "\n",
    "    def forward_pass(self, input_matrix):\n",
    "        for i in range(len(self.nn_array)):\n",
    "            layer = self.nn_array[i]\n",
    "            input_matrix = layer.batch_input(input_matrix)    \n",
    "        return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "faux_data_matrix =  np.random.randn(10, 5)\n",
    "nn = simple_neural_network(5)\n",
    "nn.add_layer(5, ReLU())\n",
    "nn.add_layer(6, SoftMax())\n",
    "out_mat = nn.forward_pass(faux_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.84732253e-03, 1.84399280e-03, 1.73772053e-04, 7.68598010e-02,\n",
       "        2.58853662e-04, 9.13016258e-01],\n",
       "       [4.63540745e-01, 1.65779837e-01, 1.76792450e-01, 1.73129511e-04,\n",
       "        3.16088113e-02, 1.62105027e-01],\n",
       "       [1.15426763e-04, 7.02030611e-03, 1.00520534e-03, 4.20149355e-01,\n",
       "        5.51771388e-03, 5.66191993e-01],\n",
       "       [1.85154268e-01, 7.10233744e-04, 1.17191502e-03, 1.02023122e-01,\n",
       "        4.66327062e-04, 7.10474135e-01],\n",
       "       [5.05482165e-01, 9.51734683e-02, 2.45448503e-01, 1.45520171e-03,\n",
       "        4.70641107e-02, 1.05376551e-01],\n",
       "       [2.99494652e-01, 1.82709447e-01, 2.36512918e-01, 7.84448563e-03,\n",
       "        1.07335271e-01, 1.66103227e-01],\n",
       "       [5.54970602e-03, 4.40722319e-04, 4.73001142e-04, 8.23329595e-01,\n",
       "        7.49987656e-04, 1.69456988e-01],\n",
       "       [1.37724842e-06, 2.43585709e-08, 3.47128345e-09, 8.46065498e-01,\n",
       "        1.41505131e-08, 1.53933082e-01],\n",
       "       [3.91256541e-01, 2.10166402e-01, 1.90672995e-01, 1.58430585e-04,\n",
       "        4.08031980e-02, 1.66942433e-01],\n",
       "       [1.34201392e-02, 5.64193173e-02, 5.25468997e-02, 5.37137175e-01,\n",
       "        8.71375696e-02, 2.53338900e-01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.12138297e-83, 1.12138297e-83, 1.12138297e-83, 1.00000000e+00,\n",
       "        8.28596168e-83, 8.28596168e-83]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp = ReLU.ReLU()\n",
    "outp.forward([-1,-1,-1,1,1,1])\n",
    "\n",
    "inp = SoftMax.SoftMax()\n",
    "inp.forward([[-1,-1,-1,190,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06484056 0.05385979 0.21175089 0.00852724]\n",
      "[[0.06484056 0.68007258 0.08796346 0.09690622 0.07021718]\n",
      " [0.05385979 0.19240152 0.08537821 0.14138571 0.52697477]\n",
      " [0.21175089 0.58970521 0.00924564 0.09109485 0.09820341]\n",
      " [0.00852724 0.21931281 0.07069741 0.23087648 0.47058606]]\n"
     ]
    }
   ],
   "source": [
    "faux_data_matrix =  np.random.randn(4, 5)\n",
    "sft_mx = SoftMax()\n",
    "print(sft_mx.forward(faux_data_matrix)[[0,1,2,3], [0,0,0,0]])\n",
    "print(sft_mx.forward(faux_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code Structure\n",
    "\n",
    "# Module: Activation Functions\n",
    "\n",
    "# Module: Neural Networks\n",
    "# nn_layer\n",
    "#   function backwards\n",
    "#   --> inputs are (loss matrix, i.e. gradient for each neuron)\n",
    "#   --> gradient of loss function d/dx activ_func(x)\n",
    "#   --> update weight x_i = d/dx_i activ_func(x) for each weight in a neuron\n",
    "#   --> \n",
    "\n",
    "# nn_full_simple\n",
    "# figure out how to numerical differentiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
