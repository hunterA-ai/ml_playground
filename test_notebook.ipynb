{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Package Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "## Custom Module Imports\n",
    "from activation_functions.SoftMax import SoftMax\n",
    "from activation_functions.ReLU import ReLU\n",
    "from loss_functons.mean_square_error import mean_square_error\n",
    "from activation_functions.LeakyReLU import LeakyReLU\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class nn_layer:\n",
    "    \"\"\"\n",
    "    Represents a weight matrix (rows, cols) = (num_neurons, input_size)\n",
    "    num_neurons is the number of neurons we wish to put in this layer\n",
    "    input_size is the fixed value defined by the last layer's outputs\n",
    "\n",
    "    The relationship between input size and number of neurons for multiple layers is ---\n",
    "    input_size = num_neurons_prev\n",
    "    input_size_next/output_size = num_neurons\n",
    "    ... etc\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_neurons, activ_func):\n",
    "        self.num_in_n = input_size\n",
    "        self.num_out_n = num_neurons\n",
    "        # self.weight_matrix = np.array([np.random.rand(input_size) for _ in range(num_neurons)])\n",
    "        # Above line has been upgraded to line below\n",
    "        self.weight_matrix = np.random.randn(self.num_out_n, self.num_in_n)\n",
    "        self.bias = np.random.rand(self.num_out_n)\n",
    "        self.activation_func = activ_func\n",
    "\n",
    "    def batch_input(self, input_matrix):\n",
    "        self.input_matrix = input_matrix\n",
    "        self.batch_size = input_matrix.shape[0]\n",
    "        \"\"\"\n",
    "        Returns the matrix product [input_matrix] * [weight_matrix]^T of dimensions\n",
    "        (batch_size, num_in_neurons) * (num_in_neurons, num_out_neurons) = (batch_size, num_out_neurons)\n",
    "        Where the output columns of the matrix are the output of the i^{th} layer of neurons\n",
    "\n",
    "        \n",
    "        (batch_size, num_out_neurons) + (num_out_neurons) is XW^T + bias, where the bias is added row-wise (to each row/neuron layer)\n",
    "        \"\"\"     \n",
    "        self.raw_output = np.dot(self.input_matrix, self.weight_matrix.T) + self.bias\n",
    "        self.activation_output = self.activation_func.forward(self.raw_output)\n",
    "        return self.activation_output\n",
    "    \n",
    "    def backward(self, error_vector):\n",
    "        \"\"\"\n",
    "        Given the error vector dC/da^(l), returns the new error vector for the next layer, dC/da^(l-1)\n",
    "        C = cost func\n",
    "        a^(l) = vector of activation functions at layer l, dim(a^(l))=num_neurons\n",
    "        z = w*x + b\n",
    "\n",
    "        Individual parials:\n",
    "        dC/da_i = error_vector_i = (1)\n",
    "        da_i/dz_i = self.activation_func.derivative(raw_output[:, i]) = (batch_size,)\n",
    "        dz_i/dw_ij = (X_1j, X_2j, ..., X_num_inputsj) = self.input_matrix[:, j] = (batch_size,)\n",
    "        \n",
    "        dC/da = error_vector = (1,)\n",
    "        da/dz = a_prime = self.activation_func.derivative(raw_output) = (batch_size, num_out_neurons)\n",
    "        dz/dw = z_prime = self.input_matrix = (batch_size, num_in_neurons)\n",
    "\n",
    "        np.outer()\n",
    "\n",
    "        col_avg ( dC/da * da/dz * dz/dw ) = Grad = (num_out_n)\n",
    "        \"\"\"\n",
    "        print('batch_size', self.batch_size)\n",
    "        print('num_n_in', self.num_in_n)\n",
    "        print('num_n_out', self.num_out_n)\n",
    "        print('raw output shape', self.raw_output.shape)\n",
    "        print('input matrix shape', self.input_matrix.shape)\n",
    "        print('activation output shape', self.activation_output.shape)\n",
    "\n",
    "        eta = 0.01\n",
    "        self.num_in_n\n",
    "        self.num_out_n\n",
    "        self.batch_size\n",
    "        dC_da_1 = error_vector # derivative of cost wrt activation function at current layer, a vector indicating the change in cost at this (num_out_n)\n",
    "        da_dz = self.activation_func.derivative(self.raw_output) # (batch_size, num_out_n)\n",
    "        dC_dz = da_dz * dC_da_1 # (batch_size, num_out_n)\n",
    "        dz_dw = self.input_matrix # (batch_size, num_in_n)\n",
    "        # print('da_dz of (batch_size, num_out_n)', da_dz.shape)\n",
    "        # print('dz_dw of (batch_size, num_in_n)', dz_dw.shape)\n",
    "        \n",
    "        # Below computes tensor dot product along specified axes, here we compute the dot product of tensors along (axis 0,axis 0), then sum along the axis.\n",
    "        # Note these axis have to be the same length\n",
    "        # Description of np.tensordot\n",
    "        # axes=0 gives outer product\n",
    "        # axes=1 gives inner product\n",
    "        # axes=2 gives tensor contraction\n",
    "        dC_dw_avg = np.tensordot(dC_dz, dz_dw, axes=(0,0)) / self.batch_size  # (num_out_n, num_in_n) = dim(W)\n",
    "        # print('da_dw of size (num_out_n, num_in_n)', da_dw.shape)\n",
    "        print('dC_dz shape', dC_dz.shape, 'W shape', self.weight_matrix.shape)\n",
    "        dC_da_0 = np.matmul(dC_dz, self.weight_matrix) # (batch_size, num_in_n)\n",
    "        print('dC_da_0 shape (batch_size, num_out_n)', dC_da_0.shape)\n",
    "        # sum average gradient across all batches\n",
    "        dC_da_0_avg = np.sum(dC_da_0, axis=0) / self.batch_size # (num_out_n)\n",
    "        self.weight_matrix = self.weight_matrix - eta * dC_dw_avg\n",
    "        return dC_da_0_avg\n",
    "    \n",
    "\n",
    "class simple_neural_network:\n",
    "    \"\"\"\n",
    "    Represents a neural network as an array of 'nn_layer' objects\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, loss_func):\n",
    "        self.nn_array = []\n",
    "        self.input_size = input_size\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def add_layer(self, num_neurons, activ_func):\n",
    "        \"\"\"\n",
    "        New layer must have input size corresponding to previous layer's output size\n",
    "        num_neurons - is the number of neurons in the current layer\n",
    "        activ_func - is the activation function that should be applied to the outputs of this layer\n",
    "        \"\"\"\n",
    "        if(len(self.nn_array) == 0):\n",
    "            self.nn_array.append(nn_layer(self.input_size, num_neurons, activ_func))\n",
    "        else:\n",
    "            prev_output_size = self.nn_array[-1].weight_matrix.shape[0]\n",
    "            self.nn_array.append(nn_layer(\n",
    "                input_size = prev_output_size, \n",
    "                num_neurons = num_neurons, \n",
    "                activ_func=activ_func))\n",
    "\n",
    "\n",
    "    def describe_network(self):\n",
    "        # weight matrix shape is (num_neurons, input_size)\n",
    "        for layer in self.nn_array:\n",
    "            print(layer)\n",
    "\n",
    "    def forward_pass(self, input_matrix):\n",
    "        for i in range(len(self.nn_array)):\n",
    "            layer = self.nn_array[i]\n",
    "            input_matrix = layer.batch_input(input_matrix)    \n",
    "        return input_matrix\n",
    "    \n",
    "    def backward_pass(self, error_vector):\n",
    "        for i in range(len(self.nn_array), 0, -1):\n",
    "            layer = self.nn_array[i-1]\n",
    "            error_vector = layer.backward(error_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 33393.3951489082\n",
      "dC/da -324.240190150393\n",
      "batch_size 10\n",
      "num_n_in 10\n",
      "num_n_out 1\n",
      "raw output shape (10, 1)\n",
      "input matrix shape (10, 10)\n",
      "activation output shape (10, 1)\n",
      "KNOW SHAPE (10, 1) (10, 1)\n",
      "da_dz shape (10, 1) W shape (1, 10)\n",
      "da_da shape (batch_size, num_out_n) (10, 10)\n",
      "batch_size 10\n",
      "num_n_in 20\n",
      "num_n_out 10\n",
      "raw output shape (10, 10)\n",
      "input matrix shape (10, 20)\n",
      "activation output shape (10, 10)\n",
      "KNOW SHAPE (10, 10) (10, 10)\n",
      "da_dz shape (10, 10) W shape (10, 20)\n",
      "da_da shape (batch_size, num_out_n) (10, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\test_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMSE Loss:\u001b[39m\u001b[39m'\u001b[39m, mse_func\u001b[39m.\u001b[39mcompute(y_true\u001b[39m=\u001b[39my_true, y_pred\u001b[39m=\u001b[39my_pred))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdC/da\u001b[39m\u001b[39m'\u001b[39m, mse_func\u001b[39m.\u001b[39mderivative(y_true, y_pred))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m nn\u001b[39m.\u001b[39;49mbackward_pass(mse_func\u001b[39m.\u001b[39;49mderivative(y_true, y_pred))\n",
      "\u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\test_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_array), \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_array[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     error_vector \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(error_vector)\n",
      "\u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\test_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mda_da shape (batch_size, num_out_n)\u001b[39m\u001b[39m'\u001b[39m, da_da\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m# sum average gradient across all batches\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m dC_da_0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msum(da_da, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39m*\u001b[39;49m (dC_da_1 \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size) \u001b[39m# (num_out_n)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_matrix \u001b[39m-\u001b[39m eta \u001b[39m*\u001b[39m dC_dw\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W2sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dC_da_0\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,) (10,) "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "diabetes = load_diabetes()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(diabetes.data)\n",
    "X_transformed = scaler.transform(diabetes.data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, diabetes.target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "nn = simple_neural_network(10, loss_func=mean_square_error())\n",
    "nn.add_layer(20, LeakyReLU(0.01))\n",
    "nn.add_layer(10, LeakyReLU(0.01))\n",
    "nn.add_layer(1, LeakyReLU(0.01))\n",
    "out_mat = nn.forward_pass(X_train)\n",
    "\n",
    "# batch 1\n",
    "mse_func = mean_square_error()\n",
    "X_train_batches = [X_train[i: i+10] for i in range(0, len(X_train), 10)]\n",
    "y_train_batches = [y_train[i:i+10] for i in range(0, len(y_train), 10)]\n",
    "\n",
    "y_pred = np.ravel(nn.forward_pass(X_train_batches[0]))\n",
    "y_true = y_train_batches[0]\n",
    "print('MSE Loss:', mse_func.compute(y_true=y_true, y_pred=y_pred))\n",
    "print('dC/da', mse_func.derivative(y_true, y_pred))\n",
    "nn.backward_pass(mse_func.derivative(y_true, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\test_notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMSE Loss:\u001b[39m\u001b[39m'\u001b[39m, mse_func\u001b[39m.\u001b[39mcompute(y_true\u001b[39m=\u001b[39my_true, y_pred\u001b[39m=\u001b[39my_pred))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdC/da\u001b[39m\u001b[39m'\u001b[39m, mse_func\u001b[39m.\u001b[39mderivative(y_true, y_pred))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m nn\u001b[39m.\u001b[39mbackward_pass(mse_func\u001b[39m.\u001b[39mderivative(y_true, y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "print('MSE Loss:', mse_func.compute(y_true=y_true, y_pred=y_pred))\n",
    "print('dC/da', mse_func.derivative(y_true, y_pred))\n",
    "nn.backward_pass(mse_func.derivative(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  7  9]\n",
      " [ 7 14 28]\n",
      " [ 9 28 66]]\n",
      "[ 4  6 10]\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "nn = simple_neural_network(10, loss_func=mean_square_error())\n",
    "nn.add_layer(20, LeakyReLU(0.01))\n",
    "nn.add_layer(10, LeakyReLU(0.01))\n",
    "nn.add_layer(1, LeakyReLU(0.01))\n",
    "# batch 1\n",
    "mse_func = mean_square_error()\n",
    "X_train_batches = [X_train[i: i+10] for i in range(0, len(X_train), 10)]\n",
    "y_train_batches = [y_train[i:i+10] for i in range(0, len(y_train), 10)]\n",
    "\n",
    "y_pred = np.ravel(nn.forward_pass(X_train_batches[0]))\n",
    "y_true = y_train_batches[0]\n",
    "print('MSE Loss:', mse_func.compute(y_true=y_true, y_pred=y_pred))\n",
    "print('dC/da', mse_func.derivative(y_true, y_pred))\n",
    "nn.backward_pass(mse_func.derivative(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   3   7]\n",
      " [  3  34  96]\n",
      " [  7  96 274]]\n",
      "[ 1.  8. 22.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.416666666666666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.hstack((X_train, y_train))\n",
    "A = np.array([[1,3,7], [0, -5, -15], [0,0,0]])\n",
    "print(np.matmul(A.T, A))\n",
    "print(np.dot(A.T, np.array([1,-1,1/5])))\n",
    "\n",
    "\n",
    "A = np.array([[1,1], [4,1], [8,1], [11,1]])\n",
    "np.matmul(A.T, A)\n",
    "np.dot(A.T, np.array([1,2,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "(5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4,  8],\n",
       "       [ 8, 16],\n",
       "       [12, 24]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.array([[1,2,3],[10,10,10],[1,2,3],[1,2,3],[1,2,3]])\n",
    "t2 = np.array([[1,2], [0,0], [1,2], [1,2], [1,2]])\n",
    "\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "t3 = np.array([np.outer(x,y) for x,y in zip(t1, t2)])\n",
    "#Below computes tensor dot product along specified axes, here we compute the dot product of tensors along (t1 axis 0, t2 axis 0), then sum along the axis.\n",
    "# np.tensordot discrepency\n",
    "# axes=0 gives outer product\n",
    "# axes=1 gives inner product\n",
    "# axes=2 gives tensor contraction\n",
    "np.tensordot(t1, t2, axes=(0,0)) \n",
    "\n",
    "# v1 = np.array([1,2,3,4,5])\n",
    "# v2 = np.array([1,1,1,1,1])\n",
    "# np.tensordot(v1,v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29738.37434644613"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_func.compute(y_true=y_train, y_pred=np.ravel(out_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 33677.99338953781\n",
      "dC/da -327.68233811957066\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "nn = simple_neural_network(10, loss_func=mean_square_error())\n",
    "nn.add_layer(20, LeakyReLU(0.01))\n",
    "nn.add_layer(10, LeakyReLU(0.01))\n",
    "nn.add_layer(1, LeakyReLU(0.01))\n",
    "# batch 1\n",
    "mse_func = mean_square_error()\n",
    "X_train_batches = [X_train[i: i+10] for i in range(0, len(X_train), 10)]\n",
    "y_train_batches = [y_train[i:i+10] for i in range(0, len(y_train), 10)]\n",
    "\n",
    "y_pred = np.ravel(nn.forward_pass(X_train_batches[0]))\n",
    "y_true = y_train_batches[0]\n",
    "print('MSE Loss:', mse_func.compute(y_true=y_true, y_pred=y_pred))\n",
    "print('dC/da', mse_func.derivative(y_true, y_pred))\n",
    "nn.backward_pass(mse_func.derivative(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.21852714  0.21872944  0.71768446]\n",
      " [-0.28915268 -0.73782518 -0.13573518]\n",
      " [ 1.82579171  0.23179451  0.22917273]\n",
      " [-2.55440273  1.35334641  0.66002298]\n",
      " [-1.99393952  1.07318724 -0.46297956]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.21852714, 0.21872944, 0.71768446],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [1.82579171, 0.23179451, 0.22917273],\n",
       "       [0.        , 1.35334641, 0.66002298],\n",
       "       [0.        , 1.07318724, 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testing = np.random.randn(5,3)\n",
    "relu_func = ReLU()\n",
    "print(X_testing)\n",
    "relu_func.forward(X_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.50213451, 0.41024829, 0.7674119 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_testing, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.80409741, -7.32605188, -3.5663163 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_testing - np.max(X_testing, axis=0, keepdims=True), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36939791, 0.108236  , 0.52236608],\n",
       "       [0.27441081, 0.20518464, 0.52040455],\n",
       "       [0.56091167, 0.32022445, 0.11886388],\n",
       "       [0.4025412 , 0.38272902, 0.21472978],\n",
       "       [0.62157997, 0.10690752, 0.27151251]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftmx = SoftMax()\n",
    "sftmx.forward(X_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1001067 , -1.36177439, -0.41402302],\n",
       "       [-1.0133265 , -0.14221217, -0.18116488],\n",
       "       [-0.26464783,  0.70709005, -0.90289345],\n",
       "       [-0.59329777, -0.57996122,  0.21845546],\n",
       "       [-0.53239851,  1.48729854, -1.19441311]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Structure\n",
    "\n",
    "# Module: Activation Functions\n",
    "\n",
    "# Module: Neural Networks\n",
    "# nn_layer\n",
    "#   function backwards\n",
    "#   --> inputs are (loss matrix, i.e. gradient for each neuron)\n",
    "#   --> gradient of loss function d/dx activ_func(x)\n",
    "#   --> update weight x_i = d/dx_i activ_func(x) for each weight in a neuron\n",
    "#   --> \n",
    "\n",
    "# nn_full_simple\n",
    "# figure out how to numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -2.0\n"
     ]
    }
   ],
   "source": [
    "mse_func = mean_square_error()\n",
    "mse = mse_func.compute(np.ones(5), np.zeros(5))\n",
    "mse_deriv = mse_func.derivative(np.ones(5), np.zeros(5))\n",
    "\n",
    "print(mse, mse_deriv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
