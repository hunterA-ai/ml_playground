{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Package Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "## Custom Module Imports\n",
    "from activation_functions.SoftMax import SoftMax\n",
    "from activation_functions.ReLU import ReLU\n",
    "from loss_functons.mean_square_error import mean_square_error\n",
    "from activation_functions.LeakyReLU import LeakyReLU\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class nn_layer:\n",
    "    \"\"\"\n",
    "    Represents a weight matrix (rows, cols) = (num_neurons, input_size)\n",
    "    num_neurons is the number of neurons we wish to put in this layer\n",
    "    input_size is the fixed value defined by the last layer's outputs\n",
    "\n",
    "    The relationship between input size and number of neurons for multiple layers is ---\n",
    "    input_size = num_neurons_prev\n",
    "    input_size_next/output_size = num_neurons\n",
    "    ... etc\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_neurons, activ_func):\n",
    "        self.num_in_n = input_size\n",
    "        self.num_out_n = num_neurons\n",
    "        # self.weight_matrix = np.array([np.random.rand(input_size) for _ in range(num_neurons)])\n",
    "        # Above line has been upgraded to line below\n",
    "        self.weight_matrix = np.random.randn(self.num_out_n, self.num_in_n)\n",
    "        self.bias = np.random.rand(self.num_out_n)\n",
    "        self.activation_func = activ_func\n",
    "\n",
    "    def batch_input(self, input_matrix):\n",
    "        self.input_matrix = input_matrix\n",
    "        self.batch_size = input_matrix.shape[0]\n",
    "        \"\"\"\n",
    "        Returns the matrix product [input_matrix] * [weight_matrix]^T of dimensions\n",
    "        (batch_size, num_in_neurons) * (num_in_neurons, num_out_neurons) = (batch_size, num_out_neurons)\n",
    "        Where the output columns of the matrix are the output of the i^{th} layer of neurons\n",
    "\n",
    "        \n",
    "        (batch_size, num_out_neurons) + (num_out_neurons) is XW^T + bias, where the bias is added row-wise (to each row/neuron layer)\n",
    "        \"\"\"     \n",
    "        self.raw_output = np.dot(self.input_matrix, self.weight_matrix.T) + self.bias_vector\n",
    "        self.activation_output = self.activation_func.forward(self.raw_output)\n",
    "        return self.activation_output\n",
    "    \n",
    "    def backward(self, error_vector):\n",
    "        \"\"\"\n",
    "        Given the error vector dC/da^(l), returns the new error vector for the next layer, dC/da^(l-1)\n",
    "        C = cost func\n",
    "        a^(l) = vector of activation functions at layer l, dim(a^(l))=num_neurons\n",
    "        z = w*x + b\n",
    "\n",
    "        Individual parials:\n",
    "        dC/da_i = error_vector_i = (1)\n",
    "        da_i/dz_i = self.activation_func.derivative(raw_output[:, i]) = (batch_size,)\n",
    "        dz_i/dw_ij = (X_1j, X_2j, ..., X_num_inputsj) = self.input_matrix[:, j] = (batch_size,)\n",
    "        \n",
    "        dC/da = error_vector = (1,)\n",
    "        da/dz = a_prime = self.activation_func.derivative(raw_output) = (batch_size, num_out_neurons)\n",
    "        dz/dw = z_prime = self.input_matrix = (batch_size, num_in_neurons)\n",
    "\n",
    "        np.outer()\n",
    "\n",
    "        col_avg ( dC/da * da/dz * dz/dw ) = Grad = (num_out_n)\n",
    "        \"\"\"\n",
    "\n",
    "        eta = 0.01\n",
    "        self.num_in_n\n",
    "        self.num_out_n\n",
    "        self.batch_size\n",
    "        dC_da_1 = error_vector # derivative of cost wrt activation function at current layer, a vector indicating the change in cost at this \n",
    "        da_dz = self.activation_func.derivative(self.raw_output) # (batch_size, num_in_n)\n",
    "        dz_dw = self.input_matrix # (batch_size, num_out_n)\n",
    "\n",
    "        # Below computes tensor dot product along specified axes, here we compute the dot product of tensors along (axis 0,axis 0), then sum along the axis.\n",
    "        # Note these axis have to be the same length\n",
    "        # Description of np.tensordot\n",
    "        # axes=0 gives outer product\n",
    "        # axes=1 gives inner product\n",
    "        # axes=2 gives tensor contraction\n",
    "        da_dw = np.tensordot(dz_dw, da_dz, axes=(0,0)) / self.batch_size  # (num_out_n, num_in_n) = dim(W)\n",
    "        dC_dw = da_dw * error_vector \n",
    "        \n",
    "        \n",
    "        da_da = np.matmul(da_dz, self.weight_matrix.T) # (batch_size, num_out_n)\n",
    "        # sum average gradient across all batches\n",
    "        dC_da_0 = np.sum(da_da, axis=0) * (dC_da_1 / self.batch_size) # (num_out_n)\n",
    "        self.weight_matrix = self.weight_matrix - eta * dC_dw\n",
    "        return dC_da_0\n",
    "    \n",
    "\n",
    "class simple_neural_network:\n",
    "    \"\"\"\n",
    "    Represents a neural network as an array of 'nn_layer' objects\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, loss_func):\n",
    "        self.nn_array = []\n",
    "        self.input_size = input_size\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def add_layer(self, num_neurons, activ_func):\n",
    "        \"\"\"\n",
    "        New layer must have input size corresponding to previous layer's output size\n",
    "        num_neurons - is the number of neurons in the current layer\n",
    "        activ_func - is the activation function that should be applied to the outputs of this layer\n",
    "        \"\"\"\n",
    "        if(len(self.nn_array) == 0):\n",
    "            self.nn_array.append(nn_layer(self.input_size, num_neurons, activ_func))\n",
    "        else:\n",
    "            prev_output_size = self.nn_array[-1].weight_matrix.shape[0]\n",
    "            self.nn_array.append(nn_layer(\n",
    "                input_size = prev_output_size, \n",
    "                num_neurons = num_neurons, \n",
    "                activ_func=activ_func))\n",
    "\n",
    "\n",
    "    def describe_network(self):\n",
    "        # weight matrix shape is (num_neurons, input_size)\n",
    "        for layer in self.nn_array:\n",
    "            print(layer)\n",
    "\n",
    "    def forward_pass(self, input_matrix):\n",
    "        for i in range(len(self.nn_array)):\n",
    "            layer = self.nn_array[i]\n",
    "            input_matrix = layer.batch_input(input_matrix)    \n",
    "        return input_matrix\n",
    "    \n",
    "    def backward_pass(self, error_vector):\n",
    "        for i in range(len(self.nn_array)-1, -1, -1):\n",
    "            layer = self.nn_array[i]\n",
    "            error_vector = layer.backwards(error_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(np.ravel(np.outer(np.array([1,1,1]), np.array([1,0,0,0]))), (3,4))\n",
    "\n",
    "np.reshape(2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "(5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4,  8],\n",
       "       [ 8, 16],\n",
       "       [12, 24]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.array([[1,2,3],[10,10,10],[1,2,3],[1,2,3],[1,2,3]])\n",
    "t2 = np.array([[1,2], [0,0], [1,2], [1,2], [1,2]])\n",
    "\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "t3 = np.array([np.outer(x,y) for x,y in zip(t1, t2)])\n",
    "#Below computes tensor dot product along specified axes, here we compute the dot product of tensors along (t1 axis 0, t2 axis 0), then sum along the axis.\n",
    "# np.tensordot discrepency\n",
    "# axes=0 gives outer product\n",
    "# axes=1 gives inner product\n",
    "# axes=2 gives tensor contraction\n",
    "np.tensordot(t1, t2, axes=(0,0)) \n",
    "\n",
    "# v1 = np.array([1,2,3,4,5])\n",
    "# v2 = np.array([1,1,1,1,1])\n",
    "# np.tensordot(v1,v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "nn = simple_neural_network(10, loss_func=mean_square_error())\n",
    "nn.add_layer(20, LeakyReLU(0.01))\n",
    "nn.add_layer(10, LeakyReLU(0.01))\n",
    "nn.add_layer(1, LeakyReLU(0.01))\n",
    "out_mat = nn.forward_pass(X_train)\n",
    "\n",
    "# batch 1\n",
    "mse_func = mean_square_error()\n",
    "X_b1 = out_mat.T[0][:10]\n",
    "y_b1 = y_test[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-311.47141262434855"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_func.derivative(y_b1, X_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.21852714  0.21872944  0.71768446]\n",
      " [-0.28915268 -0.73782518 -0.13573518]\n",
      " [ 1.82579171  0.23179451  0.22917273]\n",
      " [-2.55440273  1.35334641  0.66002298]\n",
      " [-1.99393952  1.07318724 -0.46297956]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.21852714, 0.21872944, 0.71768446],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [1.82579171, 0.23179451, 0.22917273],\n",
       "       [0.        , 1.35334641, 0.66002298],\n",
       "       [0.        , 1.07318724, 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testing = np.random.randn(5,3)\n",
    "relu_func = ReLU()\n",
    "print(X_testing)\n",
    "relu_func.forward(X_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.50213451, 0.41024829, 0.7674119 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_testing, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.80409741, -7.32605188, -3.5663163 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_testing - np.max(X_testing, axis=0, keepdims=True), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36939791, 0.108236  , 0.52236608],\n",
       "       [0.27441081, 0.20518464, 0.52040455],\n",
       "       [0.56091167, 0.32022445, 0.11886388],\n",
       "       [0.4025412 , 0.38272902, 0.21472978],\n",
       "       [0.62157997, 0.10690752, 0.27151251]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftmx = SoftMax()\n",
    "sftmx.forward(X_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1001067 , -1.36177439, -0.41402302],\n",
       "       [-1.0133265 , -0.14221217, -0.18116488],\n",
       "       [-0.26464783,  0.70709005, -0.90289345],\n",
       "       [-0.59329777, -0.57996122,  0.21845546],\n",
       "       [-0.53239851,  1.48729854, -1.19441311]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Structure\n",
    "\n",
    "# Module: Activation Functions\n",
    "\n",
    "# Module: Neural Networks\n",
    "# nn_layer\n",
    "#   function backwards\n",
    "#   --> inputs are (loss matrix, i.e. gradient for each neuron)\n",
    "#   --> gradient of loss function d/dx activ_func(x)\n",
    "#   --> update weight x_i = d/dx_i activ_func(x) for each weight in a neuron\n",
    "#   --> \n",
    "\n",
    "# nn_full_simple\n",
    "# figure out how to numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -2.0\n"
     ]
    }
   ],
   "source": [
    "mse_func = mean_square_error()\n",
    "mse = mse_func.compute(np.ones(5), np.zeros(5))\n",
    "mse_deriv = mse_func.derivative(np.ones(5), np.zeros(5))\n",
    "\n",
    "print(mse, mse_deriv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
