{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Package Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "## Custom Module Imports\n",
    "from activation_functions.SoftMax import SoftMax\n",
    "from activation_functions.ReLU import ReLU\n",
    "from loss_functons.mean_square_error import mean_square_error\n",
    "from activation_functions.LeakyReLU import LeakyReLU\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    Represents a dense layer of (num_neurons) neurons, as a weight matrix (W) of shape(out_n, in_n) and bias (b) of shape(out_n)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_neurons, activ_func):\n",
    "        self.num_in_n = input_size\n",
    "        self.num_out_n = num_neurons\n",
    "        # self.weight_matrix = np.array([np.random.rand(input_size) for _ in range(num_neurons)])\n",
    "        # Above line has been upgraded to line below\n",
    "        self.W = np.random.randn(self.num_out_n, self.num_in_n)/2\n",
    "        self.b = np.random.randn(self.num_out_n)/2\n",
    "        self.activation_func = activ_func\n",
    "\n",
    "    def batch_input(self, X):\n",
    "        \"\"\"\n",
    "        Returns the matrix product [input_matrix] * [weight_matrix]^T of dimensions\n",
    "        (batch_size, num_in_neurons) * (num_in_neurons, num_out_neurons) = (batch_size, num_out_neurons)\n",
    "\n",
    "        \n",
    "        XW^T + b is (batch_size, num_out_neurons) + (num_out_neurons), where the bias is brodcast for each row\n",
    "        \"\"\"    \n",
    "        self.X = X\n",
    "        self.batch_size = X.shape[0] \n",
    "        self.raw_output = np.dot(self.X, self.W.T) + self.b\n",
    "        self.activation_output = self.activation_func.forward(self.raw_output)\n",
    "        return self.activation_output\n",
    "    \n",
    "    \n",
    "    def backward(self, error_matrix, learning_rate):\n",
    "        \"\"\"\n",
    "        Given the error vector dC/da^(l), returns the new error vector for the next layer, dC/da^(l-1)\n",
    "\n",
    "        C = cost func\n",
    "        a^(l) = activation function at layer l\n",
    "        z = XW^T + b\n",
    "        \"\"\"\n",
    "        eta = learning_rate\n",
    "\n",
    "        dC_da_1 = error_matrix # (batch_size, out_n)\n",
    "        da_dz = self.activation_func.derivative(self.raw_output) # (batch_size, out_n)\n",
    "        dC_dz = dC_da_1 * da_dz # (batch_size, num_out_n)\n",
    "\n",
    "        # Error Gradient\n",
    "        dC_dX = np.tensordot(dC_dz, self.W, axes=(1,0)) # (batch_size, in_n)\n",
    "        # Gradient of W (average weight at w)\n",
    "        dC_dw = np.sum(np.matmul(dC_dz.T , self.X), axis=0) / self.batch_size # (out_n)\n",
    "\n",
    "        # Gradient of b\n",
    "        dC_db = np.sum(dC_dz, axis=0) / self.batch_size # (out_n)\n",
    "\n",
    "        self.W = self.W - (eta * dC_dw)\n",
    "        self.b = self.b - (eta * dC_db)\n",
    "        return dC_dX\n",
    "    \n",
    "\n",
    "class OutLayer:\n",
    "    \"\"\"\n",
    "    Represents the output layer of a neural network as a weight vector (w) with shape(in_n) and scalar bias (b)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, loss_func):\n",
    "        self.num_in_n = input_size\n",
    "        self.loss_func = loss_func\n",
    "        self.num_out_n = 1\n",
    "        self.W = np.random.randn(self.num_in_n)/2\n",
    "        self.b = np.random.randn(self.num_out_n)/2\n",
    "\n",
    "    def batch_input(self, X):\n",
    "        self.X = X\n",
    "        self.batch_size = X.shape[0]\n",
    "        \"\"\"\n",
    "        Returns the matrix product [input_matrix] * [weight_matrix]^T of dimensions\n",
    "        (batch_size, num_in_neurons) * (num_in_neurons, num_out_neurons) = (batch_size, num_out_neurons)\n",
    "\n",
    "\n",
    "        XW^T + bias is (batch_size, num_out_neurons) + (num_out_neurons), where the bias is brodcast for each row\n",
    "        \"\"\"     \n",
    "        self.raw_output = np.dot(self.X, self.W.T) + self.b\n",
    "        return self.raw_output\n",
    "    \n",
    "    \n",
    "    def backward(self, y_true, learning_rate):\n",
    "        \"\"\"\n",
    "        Given the error vector dC/da^(l), returns the new error vector for the next layer, dC/da^(l-1)\n",
    "\n",
    "        C = cost func\n",
    "        a^(l) = activation function at layer l\n",
    "        z = XW^T + b\n",
    "        \"\"\"\n",
    "        eta = learning_rate\n",
    "\n",
    "        # Error Gradient\n",
    "        dC_dX = self.loss_func.dC_dX(X=self.X, w=self.W, b=self.b, y=y_true)\n",
    "        # Gradient of W\n",
    "        dC_dw = self.loss_func.dC_dw(X=self.X, w=self.W, b=self.b, y=y_true)\n",
    "        # Gradient of b\n",
    "        dC_db = self.loss_func.dC_db(X=self.X, w=self.W, b=self.b, y=y_true)\n",
    "\n",
    "        self.W = self.W - (eta * dC_dw)\n",
    "        self.b = self.b - (eta * dC_db)\n",
    "        return dC_dX\n",
    "    \n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Represents a neural network as an array of {\"DenseLayer\", \"OutLayer\"} objects.\n",
    "    The last element in the array must be of type \"OutLayer\"\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, loss_func = mean_square_error()):\n",
    "        self.nn_array = []\n",
    "        self.input_size = input_size\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def add_layer(self, num_neurons, activ_func=ReLU(), type=\"dense\"):\n",
    "        \"\"\"\n",
    "        type = {'dense', 'output'}\n",
    "\n",
    "        New layer must have input size corresponding to previous layer's output size\n",
    "        num_neurons - is the number of neurons in the current layer\n",
    "        activ_func - is the activation function that should be applied to the outputs of this layer\n",
    "        \"\"\"\n",
    "        num_in_n = 0\n",
    "        if(len(self.nn_array) == 0):\n",
    "            num_in_n = self.input_size\n",
    "        else:\n",
    "            num_in_n = self.nn_array[-1].W.shape[0]\n",
    "        \n",
    "        if(type == \"output\"):\n",
    "            self.nn_array.append(OutLayer(\n",
    "                input_size = num_in_n, \n",
    "                loss_func=self.loss_func))\n",
    "        elif(type == \"dense\"):\n",
    "            self.nn_array.append(DenseLayer(\n",
    "                input_size=num_in_n,\n",
    "                num_neurons=num_neurons,\n",
    "                activ_func=activ_func\n",
    "            ))\n",
    "        else:\n",
    "            raise(ValueError(f\"Invalid Argument {type}, expected 'dense' or 'output'\"))\n",
    "        \n",
    "        \n",
    "    def describe_network(self):\n",
    "        # weight matrix shape is (num_neurons, input_size)\n",
    "        for layer in self.nn_array:\n",
    "            print(layer)\n",
    "\n",
    "    def forward_pass(self, input_matrix):\n",
    "        for i in range(len(self.nn_array)):\n",
    "            layer = self.nn_array[i]\n",
    "            input_matrix = layer.batch_input(input_matrix)    \n",
    "        return input_matrix\n",
    "    \n",
    "    def backward_pass(self, y_true,learning_rate):\n",
    "        layer = self.nn_array[-1]\n",
    "        dC_da = layer.backward(y_true, learning_rate)\n",
    "        for i in range(len(self.nn_array), 0, -1):\n",
    "            layer = self.nn_array[i-1]\n",
    "            error_vector = layer.backward(dC_da, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# diabetes = load_diabetes()\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(diabetes.data)\n",
    "# X_transformed = scaler.transform(diabetes.data)\n",
    "# scaler.fit(diabetes.target.reshape(-1,1))\n",
    "# y_transformed = scaler.transform(diabetes.target.reshape(-1,1))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_transformed, test_size=0.2, random_state=42, shuffle=True)\n",
    "# nn = SimpleNeuralNetwork(input_size=10, loss_func=mean_square_error())\n",
    "# nn.add_layer(num_neurons=8, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "# nn.add_layer(num_neurons=32, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "# nn.add_layer(num_neurons=64, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "# nn.add_layer(num_neurons=32, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "# nn.add_layer(num_neurons=8, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "# nn.add_layer(num_neurons=1, activ_func=LeakyReLU(0.01), type=\"output\")\n",
    "\n",
    "# # batches\n",
    "# mse_func = mean_square_error()\n",
    "# X_train_batches = [X_train[i: i+10] for i in range(0, len(X_train), 10)]\n",
    "# y_train_batches = [y_train[i:i+10] for i in range(0, len(y_train), 10)]\n",
    "\n",
    "# num_epochs = 100\n",
    "# for i in range(num_epochs):\n",
    "#     # Epoch\n",
    "#     # for i in range(len(X_train_batches)):\n",
    "#     for i in range(2):\n",
    "#         X_train_batch = X_train_batches[i]\n",
    "#         y_train_batch = y_train_batches[i]\n",
    "#         y_pred_batch = nn.forward_pass(X_train_batch)\n",
    "#         nn.backward_pass(mse_func.derivative(y_train_batch, y_pred_batch))\n",
    "#     y_true = y_train\n",
    "#     y_pred = nn.forward_pass(X_train)\n",
    "#     print('MSE Loss:', mse_func.compute(y_true=y_true, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing single instance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "diabetes = load_diabetes()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(diabetes.data)\n",
    "X_transformed = scaler.transform(diabetes.data)\n",
    "scaler.fit(diabetes.target.reshape(-1,1))\n",
    "y_transformed = scaler.transform(diabetes.target.reshape(-1,1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_transformed, test_size=0.2, random_state=42, shuffle=True)\n",
    "nn = SimpleNeuralNetwork(input_size=10, loss_func=mean_square_error())\n",
    "nn.add_layer(num_neurons=8, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "nn.add_layer(num_neurons=32, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "nn.add_layer(num_neurons=64, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "nn.add_layer(num_neurons=32, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "nn.add_layer(num_neurons=8, activ_func=LeakyReLU(0.01), type=\"dense\")\n",
    "nn.add_layer(num_neurons=1, activ_func=LeakyReLU(0.01), type=\"output\")\n",
    "\n",
    "\n",
    "# batches\n",
    "mse_func = mean_square_error()\n",
    "X_train_batches = [X_train[i: i+10] for i in range(0, len(X_train), 10)]\n",
    "y_train_batches = [y_train[i:i+10] for i in range(0, len(y_train), 10)]\n",
    "\n",
    "X_train_batch = X_train_batches[0]\n",
    "y_train_batch = y_train_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_mse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Prediction: 17.560213541624897\n",
      "MSE Loss: 859.2832393204329\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\test_notebook.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMax Prediction:\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mmax(np\u001b[39m.\u001b[39mabs(y_pred_batch)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMSE Loss:\u001b[39m\u001b[39m'\u001b[39m, mse_func\u001b[39m.\u001b[39mcompute(y_true\u001b[39m=\u001b[39my_train_batch, y_pred\u001b[39m=\u001b[39my_pred_batch))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m nn\u001b[39m.\u001b[39;49mbackward_pass(y_true\u001b[39m=\u001b[39;49my_train_batch, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\test_notebook.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_pass\u001b[39m(\u001b[39mself\u001b[39m, y_true,learning_rate):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_array[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m     dC_da \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(y_true, learning_rate)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_array), \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m         layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_array[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\test_notebook.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m dC_dX \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_func\u001b[39m.\u001b[39mdC_dX(X\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX, w\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW, b\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb, y\u001b[39m=\u001b[39my_true)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# Gradient of W\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m dC_dw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func\u001b[39m.\u001b[39;49mdC_dw(X\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX, w\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW, b\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb, y\u001b[39m=\u001b[39;49my_true)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# Gradient of b\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ande348/GIT%20Personal/ml_playground/test_notebook.ipynb#W5sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m dC_db \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_func\u001b[39m.\u001b[39mdC_db(X\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX, w\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW, b\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb, y\u001b[39m=\u001b[39my_true)\n",
      "File \u001b[1;32mc:\\Users\\ande348\\GIT Personal\\ml_playground\\loss_functons\\mean_square_error.py:14\u001b[0m, in \u001b[0;36mmean_square_error.dC_dw\u001b[1;34m(self, X, w, b, y)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdC_dw\u001b[39m(\u001b[39mself\u001b[39m, X, w, b, y):\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39;49mmatmul(X, (np\u001b[39m.\u001b[39;49mdot(X, w) \u001b[39m+\u001b[39;49m b \u001b[39m-\u001b[39;49m y))) \u001b[39m/\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 8)"
     ]
    }
   ],
   "source": [
    "y_pred_batch = nn.forward_pass(X_train_batch)\n",
    "loss = mse_func.compute(y_train_batch, y_pred_batch)\n",
    "stored_mse.append(loss)\n",
    "print('Max Prediction:', np.max(np.abs(y_pred_batch)))\n",
    "print('MSE Loss:', mse_func.compute(y_true=y_train_batch, y_pred=y_pred_batch))\n",
    "nn.backward_pass(y_true=y_train_batch, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "indices = list(range(len(y_pred_batch)))\n",
    "\n",
    "# Create a line plot of the loss values\n",
    "plt.plot(indices, y_pred_batch, marker='o', linestyle='-')\n",
    "plt.plot(indices, y_train_batch, marker='o', linestyle='-')\n",
    "# Add labels and a title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('y_value')\n",
    "plt.title('Predictions vs Ground Truth')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_func.derivative(y_train_batch, y_pred_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "indices = list(range(len(stored_mse)))\n",
    "\n",
    "# Create a line plot of the loss values\n",
    "plt.plot(indices, stored_mse, marker='o', linestyle='-')\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Each Index')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "sym_str = \", \".join([f'x{i}' for i in range(10)])\n",
    "print(sym_str)\n",
    "sym_ary = sym.symbols(sym_str)\n",
    "print(sym_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = np.array([[1,1,1], [2,2,2]])\n",
    "w_temp = np.array([1,10,100])\n",
    "y_temp = np.array([100,200])\n",
    "\n",
    "print(np.matmul(X_temp, w_temp))\n",
    "print(np.matmul(X_temp, w_temp) - y_temp)\n",
    "print(np.outer(np.matmul(X_temp, w_temp) - y_temp, w_temp))\n",
    "print(np.sum(np.outer(np.matmul(X_temp, w_temp) - y_temp, w_temp), axis=0)/ X_temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
