{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Package Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "## Custom Module Imports\n",
    "from activation_functions.SoftMax import SoftMax\n",
    "from activation_functions.ReLU import ReLU\n",
    "from loss_functons.mean_square_error import mean_square_error\n",
    "from activation_functions.LeakyReLU import LeakyReLU\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_layer:\n",
    "    \"\"\"\n",
    "    Represents a dense layer of a neural network, as a weight matrix W of shape(out_n, in_n)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_neurons, activ_func):\n",
    "        self.num_in_n = input_size\n",
    "        self.num_out_n = num_neurons\n",
    "        # self.weight_matrix = np.array([np.random.rand(input_size) for _ in range(num_neurons)])\n",
    "        # Above line has been upgraded to line below\n",
    "        self.W = np.random.randn(self.num_out_n, self.num_in_n)/2\n",
    "        self.bias = np.random.randn(self.num_out_n)/2\n",
    "        self.activation_func = activ_func\n",
    "\n",
    "    def batch_input(self, X):\n",
    "        self.X = X\n",
    "        self.batch_size = X.shape[0]\n",
    "        \"\"\"\n",
    "        Returns the matrix product [input_matrix] * [weight_matrix]^T of dimensions\n",
    "        (batch_size, num_in_neurons) * (num_in_neurons, num_out_neurons) = (batch_size, num_out_neurons)\n",
    "\n",
    "        \n",
    "        XW^T + bias is (batch_size, num_out_neurons) + (num_out_neurons), where the bias is brodcast for each row\n",
    "        \"\"\"     \n",
    "        self.raw_output = np.dot(self.X, self.W.T) + self.bias\n",
    "        self.activation_output = self.activation_func.forward(self.raw_output)\n",
    "        return self.activation_output\n",
    "    \n",
    "    \n",
    "    def backward(self, error_matrix, learning_rate):\n",
    "        \"\"\"\n",
    "        Given the error vector dC/da^(l), returns the new error vector for the next layer, dC/da^(l-1)\n",
    "\n",
    "        C = cost func\n",
    "        a^(l) = activation function at layer l\n",
    "        z = XW^T + b\n",
    "        \"\"\"\n",
    "        eta = learning_rate\n",
    "        self.num_in_n\n",
    "        self.num_out_n\n",
    "        self.batch_size\n",
    "        dC_da_1 = error_matrix # (batch_size, out_n)\n",
    "        da_dz = self.activation_func.derivative(self.raw_output) # (batch_size, out_n)\n",
    "        dC_dz = dC_da_1 * da_dz # (batch_size, num_out_n)\n",
    "\n",
    "        # Error Gradient\n",
    "        dC_dX = np.tensordot(dC_dz, self.W, axes=(1,0)) # (batch_size, in_n)\n",
    "        # Gradient of W (average weight at w)\n",
    "        dC_dw = np.sum(np.matmul(dC_dz , self.X.T), axis=0) / self.batch_size # (out_n)\n",
    "\n",
    "        # Gradient of b\n",
    "        dC_db = np.sum(dC_dz, axis=0) / self.batch_size # (out_n)\n",
    "\n",
    "        self.W = self.W - (eta * dC_dw)\n",
    "        self.bias = self.bias - (eta * dC_db)\n",
    "        return dC_dX\n",
    "    \n",
    "class out_layer:\n",
    "    \"\"\"\n",
    "    Represents the output layer of a neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, loss_func):\n",
    "        self.num_in_n = input_size\n",
    "        self.loss_func = loss_func\n",
    "        self.num_out_n = 1\n",
    "        self.W = np.random.randn(self.num_in_n)/2\n",
    "        self.bias = np.random.randn(self.num_out_n)/2\n",
    "\n",
    "    def batch_input(self, X):\n",
    "        self.X = X\n",
    "        self.batch_size = X.shape[0]\n",
    "        \"\"\"\n",
    "        Returns the matrix product [input_matrix] * [weight_matrix]^T of dimensions\n",
    "        (batch_size, num_in_neurons) * (num_in_neurons, num_out_neurons) = (batch_size, num_out_neurons)\n",
    "\n",
    "\n",
    "        XW^T + bias is (batch_size, num_out_neurons) + (num_out_neurons), where the bias is brodcast for each row\n",
    "        \"\"\"     \n",
    "        self.raw_output = np.dot(self.X, self.W.T) + self.bias\n",
    "        return self.raw_output\n",
    "    \n",
    "    \n",
    "    def backward(self, y_true, learning_rate):\n",
    "        \"\"\"\n",
    "        Given the error vector dC/da^(l), returns the new error vector for the next layer, dC/da^(l-1)\n",
    "\n",
    "        C = cost func\n",
    "        a^(l) = activation function at layer l\n",
    "        z = XW^T + b\n",
    "        \"\"\"\n",
    "        eta = learning_rate\n",
    "\n",
    "        # Error Gradient\n",
    "        dC_dX = self.loss_func.dMSE_dX(X=self.X, w=self.W, b=self.bias, y=y_true)\n",
    "        # Gradient of W\n",
    "        dC_dw = self.loss_func.dMSE_dW(X=self.X, w=self.W, b=self.bias, y=y_true)\n",
    "        # Gradient of b\n",
    "        dC_db = self.loss_func.dMSE_db(X=self.X, w=self.W, b=self.bias, y=y_true)\n",
    "\n",
    "        self.W = self.W - (eta * dC_dw)\n",
    "        self.bias = self.bias - (eta * dC_db)\n",
    "        return dC_dX\n",
    "    \n",
    "\n",
    "class simple_neural_network:\n",
    "    \"\"\"\n",
    "    Represents a neural network as an array of 'NN_Layer' objects\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        self.nn_array = []\n",
    "        self.input_size = input_size\n",
    "\n",
    "\n",
    "    def add_layer(self, num_neurons, activ_func, scale_inputs=False):\n",
    "        \"\"\"\n",
    "        New layer must have input size corresponding to previous layer's output size\n",
    "        num_neurons - is the number of neurons in the current layer\n",
    "        activ_func - is the activation function that should be applied to the outputs of this layer\n",
    "        \"\"\"\n",
    "        if(len(self.nn_array) == 0):\n",
    "            self.nn_array.append(dense_layer(self.input_size,\n",
    "                num_neurons, \n",
    "                activ_func))\n",
    "        else:\n",
    "            prev_output_size = self.nn_array[-1].weight_matrix.shape[0]\n",
    "            self.nn_array.append(dense_layer(\n",
    "                input_size = prev_output_size, \n",
    "                num_neurons = num_neurons, \n",
    "                activ_func=activ_func))\n",
    "\n",
    "\n",
    "    def describe_network(self):\n",
    "        # weight matrix shape is (num_neurons, input_size)\n",
    "        for layer in self.nn_array:\n",
    "            print(layer)\n",
    "\n",
    "    def forward_pass(self, input_matrix):\n",
    "        for i in range(len(self.nn_array)):\n",
    "            layer = self.nn_array[i]\n",
    "            input_matrix = layer.batch_input(input_matrix)    \n",
    "        return input_matrix\n",
    "    \n",
    "    def backward_pass(self, error_vector, learning_rate):\n",
    "        for i in range(len(self.nn_array), 0, -1):\n",
    "            layer = self.nn_array[i-1]\n",
    "            error_vector = layer.backward(error_vector, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "diabetes = load_diabetes()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(diabetes.data)\n",
    "X_transformed = scaler.transform(diabetes.data)\n",
    "scaler.fit(diabetes.target.reshape(-1,1))\n",
    "y_transformed = scaler.transform(diabetes.target.reshape(-1,1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_transformed, test_size=0.2, random_state=42, shuffle=True)\n",
    "nn = simple_neural_network(10)\n",
    "nn.add_layer(8, LeakyReLU(0.01))\n",
    "nn.add_layer(32, LeakyReLU(0.01), scale_inputs=True)\n",
    "nn.add_layer(64, LeakyReLU(0.01))\n",
    "nn.add_layer(32, LeakyReLU(0.01), scale_inputs=True)\n",
    "nn.add_layer(8, LeakyReLU(0.01), scale_inputs=True)\n",
    "nn.add_layer(1, LeakyReLU(0.01))\n",
    "\n",
    "# batches\n",
    "mse_func = mean_square_error()\n",
    "X_train_batches = [X_train[i: i+10] for i in range(0, len(X_train), 10)]\n",
    "y_train_batches = [y_train[i:i+10] for i in range(0, len(y_train), 10)]\n",
    "\n",
    "num_epochs = 100\n",
    "for i in range(num_epochs):\n",
    "    # Epoch\n",
    "    # for i in range(len(X_train_batches)):\n",
    "    for i in range(2):\n",
    "        X_train_batch = X_train_batches[i]\n",
    "        y_train_batch = y_train_batches[i]\n",
    "        y_pred_batch = nn.forward_pass(X_train_batch)\n",
    "        nn.backward_pass(mse_func.derivative(y_train_batch, y_pred_batch))\n",
    "    y_true = y_train\n",
    "    y_pred = nn.forward_pass(X_train)\n",
    "    print('MSE Loss:', mse_func.compute(y_true=y_true, y_pred=y_pred))\n",
    "\n",
    "# scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "# print('MSE Loss:', mse_func.compute(y_true=y_true, y_pred=y_pred))\n",
    "# print('dC/da', mse_func.derivative(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing single instance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "diabetes = load_diabetes()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(diabetes.data)\n",
    "X_transformed = scaler.transform(diabetes.data)\n",
    "scaler.fit(diabetes.target.reshape(-1,1))\n",
    "y_transformed = scaler.transform(diabetes.target.reshape(-1,1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_transformed, test_size=0.2, random_state=42, shuffle=True)\n",
    "nn = simple_neural_network(10)\n",
    "nn.add_layer(8, LeakyReLU(0.01))\n",
    "nn.add_layer(32, LeakyReLU(0.01), scale_inputs=True)\n",
    "nn.add_layer(64, LeakyReLU(0.01))\n",
    "nn.add_layer(32, LeakyReLU(0.01), scale_inputs=True)\n",
    "nn.add_layer(8, LeakyReLU(0.01), scale_inputs=True)\n",
    "nn.add_layer(1, LeakyReLU(0.01))\n",
    "\n",
    "# batches\n",
    "mse_func = mean_square_error()\n",
    "X_train_batches = [X_train[i: i+10] for i in range(0, len(X_train), 10)]\n",
    "y_train_batches = [y_train[i:i+10] for i in range(0, len(y_train), 10)]\n",
    "\n",
    "\n",
    "\n",
    "X_train_batch = X_train_batches[0]\n",
    "y_train_batch = y_train_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_mse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_batch = nn.forward_pass(X_train_batch)\n",
    "dC_da_final_layer = mse_func.derivative(y_train_batch, y_pred_batch)\n",
    "loss = mse_func.compute(y_train_batch, y_pred_batch)\n",
    "stored_mse.append(loss)\n",
    "print('Max Prediction:', np.max(np.abs(y_pred_batch)))\n",
    "print('MSE Loss:', mse_func.compute(y_true=y_train_batch, y_pred=y_pred_batch))\n",
    "print('dC/da', mse_func.derivative(y_train_batch, y_pred_batch))\n",
    "nn.backward_pass(dC_da_final_layer, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "indices = list(range(len(y_pred_batch)))\n",
    "\n",
    "# Create a line plot of the loss values\n",
    "plt.plot(indices, y_pred_batch, marker='o', linestyle='-')\n",
    "plt.plot(indices, y_train_batch, marker='o', linestyle='-')\n",
    "# Add labels and a title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('y_value')\n",
    "plt.title('Predictions vs Ground Truth')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_func.derivative(y_train_batch, y_pred_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "indices = list(range(len(stored_mse)))\n",
    "\n",
    "# Create a line plot of the loss values\n",
    "plt.plot(indices, stored_mse, marker='o', linestyle='-')\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Each Index')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0, x1, x2, x3, x4, x5, x6, x7, x8, x9\n",
      "(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9)\n"
     ]
    }
   ],
   "source": [
    "import sympy as sym\n",
    "sym_str = \", \".join([f'x{i}' for i in range(10)])\n",
    "print(sym_str)\n",
    "sym_ary = sym.symbols(sym_str)\n",
    "print(sym_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111 222]\n",
      "[11 22]\n",
      "[[  11  110 1100]\n",
      " [  22  220 2200]]\n",
      "[  16.5  165.  1650. ]\n"
     ]
    }
   ],
   "source": [
    "X_temp = np.array([[1,1,1], [2,2,2]])\n",
    "w_temp = np.array([1,10,100])\n",
    "y_temp = np.array([100,200])\n",
    "\n",
    "print(np.matmul(X_temp, w_temp))\n",
    "print(np.matmul(X_temp, w_temp) - y_temp)\n",
    "print(np.outer(np.matmul(X_temp, w_temp) - y_temp, w_temp))\n",
    "print(np.sum(np.outer(np.matmul(X_temp, w_temp) - y_temp, w_temp), axis=0)/ X_temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
